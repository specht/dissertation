\cleardoublepage
% ==============================================================
\chapter{Discussion and outlook}
% ==============================================================

In the scope of this thesis, several software tools have been designed, 
developed and subsequently applied for the investigation of biological 
questions, including the quantitation program qTrace for the characterization 
of the \cre~chloroplast proteome and its anaerobic response.
In addition, a re-designed version of the Genomic Peptide Finder was used
to establish an automated proteogenomic genome annotation pipeline.
These two systems have been implemented in the context of the data evaluation
platform Proteomatic.

\section{Decentralized MS/MS data evaluation infrastructure}

From the researcher's perspective, Proteomatic provides a way to create and
execute MS/MS data evaluation pipelines without the need to install each of
the individual software tools manually.
Every researcher with access to a computer is able to run data evaluation
pipelines of virtually arbritrary complexity in an independent fashion.
This independence is facilitated by the fact that the system runs on Windows,
Mac OS X, and Linux.
% ------------------------------------

\paragraph{High-throughput data processing.}

From the primary investigator's point of view, Proteomatic implements a 
decentralized data evaluation system, in which the data evaluation requirements
of every researcher can be typically accomodated for with a single computer,
resulting in optimal scalability of the system because each computer acts 
independently.
Furthermore, a distributed system as provided by Proteomatic results in 
increased robustness towards component failure and the lab-wide throughput
is not greatly diminished by one or two failing computers.
Due to the automatic software downloading and update features, a fully 
functional environment can be quickly restored on a new hardware unit.
% ------------------------------------
Therefore, Proteomatic provides an advantage in terms of system reliability 
over existing alternatives such as the Trans-Proteomics Pipeline (TPP) and 
The OpenMS Proteomics Pipeline (TOPP).
Although these systems provide more comprehensive functionality than Proteomatic
currently does, the setup and preparation of these systems to a point where
they become fully functional from the user's perspective is a time-consuming 
process.
Proteomatic performs these initial tasks automatically when they are required.
TPP requires the installation of an Apache web server for the purpose
of providing a web browser-based graphical user interface (GUI).
While the GUI is helpful for users, the required Apache web server presents a 
non-negligible security risk because it potentially renders the user's computer 
accessible from the Internet.
It can be expected that most users will be unaware of this issue and therefore
not take care to update the web server software regularly.
This issue is especially precarious on Windows and Mac OS X which do not 
natively provide centralized, automatic software updating.
Finally, because Proteomatic is easy to install, it is also beneficial in 
education, giving students the opportunity to gain hands-on MS/MS data 
evalution experience.

\paragraph{Rapid deployment of novel functionality.}

The fact that mass spectrometric data acquisition is constantly improving in
terms of quantity and quality has fueled the development of software tools for
various data evaluation-related purposes.
Most of these programs can be easily integrated into Proteomatic.
In addition, the deployment of novel, in-house developed functionality is
greatly facilitated by Proteomatic due to its support for multiple scripting
languages and the central script update mechanism.
When novel functionality is being developed, initial decisions should be made 
carefully.
Although Proteomatic can support virtually any programming language, choosing
a programming language which runs on Windows only inherently restricts the 
target audience.
Using Proteomatic, tool developers can concentrate on the actual data 
processing and do not have to spend time on creating graphical user interfaces.

% \begin{todo}
% - decentralized system
% - many researches, many computers for data evaluation
% - low impact of failing single computers on overall performance 
% - functional data evaluation setup can be restored in a straightforward way
% - automatic software downloading
% - also beneficial in education
% - two-level design (CLI and GUI) allows for usage in situations which too 
%   complex to be feasible for handling in the GUI
% - rapid deployment of novel functionality
% - support for multiple scripting languages
% 


\section{Automated quantitation of metabolically labeled samples}

One of the studies presented in this thesis provides a comprehensive list of 
experimentally deduced chloroplast proteins for \cre~for the first time
(manuscript 2).
In addition, the anaerobic response of the chloroplast proteome of the green 
alga is characterized. 
Among other results, the anaerobic induction of hydrogenase, a protein involved 
in hydrogen production, is experimentally confirmed.

The framework for high-throughput quantitation of metabolically labeled 
samples, in this case by SILAC, is provided by qTrace.
qTrace is a targeted quantitation program which takes a list of peptides
identified via MS/MS, along with their retention time, as input and then 
reconstructs the corresponding isotope envelopes which are subsequently 
matched to the full scans.
In comparison to MS/MS-based quantitation, this strategy allows for the
incorporation of a large set of data points during the final peptide ratio
estimation because peptides usually elute for a time span long enough to
show their precursor peaks in several successive full scans.
It is obvious that the assignment of precursor peaks in full scans is highly
ambiguous, given the fact that several distinct peptides can yield the
same {\em m/z} values and therefore, peptide identification is not possible
using intact peptide masses only.
In order to remove spurious precursor peak assignments, several filters are 
employed.

\paragraph{MS/MS requirement filter.}

The inherent ambiguity resulting from assigning precursor peaks in full scans
is compensated with the requirement of MS/MS identification in the same band
within a small retention time window of one minute.
The assumption made by such a {\em MS/MS requirement filter} is that once the
identity of a precursor peak has been established via MS/MS, this information
can be expected to remain true within a short time span.

\paragraph{Labeling strategies.}

\section{Advancement of the Genomic Peptide Finder }

In 2007, GPF was redesigned and implemented from scratch in the scope of this
thesis to add a variety of features \citep{Specht2011_GPF}:

\begin{itemize}
\item intron splits may occur within a single coding nucleotide triplet
\item splice donor/acceptor site consensus sequences may be specified to
reduce the number of spurious spliced peptide alignments
\item increased search speed by employing an indexing strategy while locating
the occurences of sequence tags in the genomic DNA sequence
\end{itemize}

In addition, a method for the automated validation of GPF candidate peptides,
employing standard database search programs such as OMSSA was established.
This allows for statistically robust identification of GPF-deduced peptides
alongside gene model peptides.
Furthermore, an annotation pipeline was established in which resulting GPF
peptides are passed to AUGUSTUS, which performs an {\em ab initio} gene 
model prediction supplemented by various extrinsic hint sources including
GPF peptides.
It is therefore a major contribution of this thesis that an automated
proteogenomic annotation of the \cre~genome in which MS/MS data generated
for various unrelated purposes can be re-used for the generation of
extrinsic AUGUSTUS hints has become possible.

\paragraph{GPF-supported proteogenomic genome annotation.}

\begin{SCtopfig}
\includegraphics[width=0.7\textwidth]{figures/gpf-omssa.jpg}
\caption{
{\bf Validation of GPF candidate peptides via a target/decoy approach
    using previously established gene models.} 
    Statistical significance of identified GPF candidate peptides is 
    assessed using existing gene models which may be incomplete but
    can be expected to contain a high amount of correct sequences.
}
\label{fig:gpf-omssa}
\end{SCtopfig}

The GPF annotation pipeline presented in this thesis follows a similar strategy
(\cite{Specht2011_GPF}, see p.~\pageref{section:gpf} and \pageref{paper:gpf}).
However, the GPF approach is less biased in comparison to the exon splice graph
approach because it does not require exon/intron prediction as a first step.
GPF candidate peptides are solely generated from MS/MS {\em de novo} sequencing
and subsequent mapping of the resulting peptides to the genome, using 
a user-defined maximum intron length and a set of possible splice donor/acceptor 
site consensus sequences.
This means that intron prediction is carried out on a per-peptide basis, and
all peptides are treated independently.
The actual validation of extrinsic hints and splice site detection is
carried out by AUGUSTUS in the final annotation step.
Moreover, the approach is highly flexible because no specialized database
search program is required because candidate peptides are inferred by GPF
and then passed down the evaluation pipeline alongside a protein database
(see Fig.~\ref{fig:gpf-omssa}).
Although the protein database is used to estimate the FDR of peptide 
identifications, the final GPF-deduced peptides which are exported as
peptide hints do not originate from this database, although in the case
of \cre, a big portion of these protein database peptides could be 
independently confirmed via GPF \citep{Specht2011_GPF}.


\paragraph{Proteogenomic genome annotation.}

\paragraph{Identification of novel targets for reverse-genetics approaches.}

\section{Outlook}

% outlook:
% - more functionality
% - in particular, free de novo tool
% - also provide data, not just tools
% - remote execution (but local creation)
% - refinement of the workflow interaction metaphors (batches should be defined
%   per arrow, not per input file box)
% \end{todo}
